{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ling 406 - Term Project - Marco Fonseca (Yelp reviews)\n",
    "\n",
    "The code below imports the necessary library and then it does some data processing. Reviews that have 3 or more stars are classified as positive, and reviews that have 2 or 1 stars are classified as negative. Some data processing to delete unwanted characters ('\\xa0') was also performed. The result was a list of lists. A for loop was desgined in order to create tuples in this lists of lists that group the review and its annotation together. Then, steps very similar to the movie reviews were conducted, with the difference that the \"stars\" in the beginning of the variable that is used to make the frequency distribution was made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "\n",
    "with open('all_reviews.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "\n",
    "l= data.split(']]]')\n",
    "l2 = [s.replace('[[[', '') for s in l]\n",
    "l3 = [s.replace('{{{1star}}}', '{neg}') for s in l2]\n",
    "l4 = [s.replace('{{{2star}}}', '{neg}') for s in l3]\n",
    "l5 = [s.replace('{{{3star}}}', '{pos}') for s in l4]\n",
    "l6 = [s.replace('{{{4star}}}', '{pos}') for s in l5]\n",
    "l7 = [s.replace('{{{5star}}}', '{pos}') for s in l6]\n",
    "l8 = [s.replace('\\xa0', '') for s in l7]\n",
    "\n",
    "out = []\n",
    "for strings in l8: \n",
    "    s = re.match( r'(.*)\\}(.*?) .*', strings, re.M|re.I)\n",
    "    #print(s)\n",
    "    if s : \n",
    "        first = s.group(1) + '}'\n",
    "        second = strings.replace(first,'')\n",
    "        tuple = ( second , first ) \n",
    "        out.append(tuple)\n",
    "        \n",
    "random.shuffle(out)\n",
    "words = nltk.word_tokenize(data)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for w in words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words2 = nltk.FreqDist(all_words)\n",
    "all_words3 = list(all_words2.keys())\n",
    "stars =  {\"{\", \"1star\", \"2star\", \"3star\", \"4star\", \"5star\", \"}\", '[', ']'}\n",
    "all_words4 = [e for e in all_words3 if e not in stars] #3rd set of features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below show the features employed. They were the same as the movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = all_words4\n",
    "bigram_features = list(nltk.bigrams(baseline)) #2nd set of features\n",
    "remove = {\":\", \",\", \".\", \";\", \"?\", \"!\", \"(\", \")\", \"'\", \"``\" \"to\", \"a\", \"the\", \"an\", \"on\", \"at\", \"in\"}\n",
    "content_features = [e for e in baseline if e not in remove] #3rd set of features\n",
    "bigrams_content_features =  list(nltk.bigrams(content_features))#4th set of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below have the functions that looks for the features, followed by the accuracies of each model. Steps were the same as the ones employed in the movie reviews, including 20% of the data for training data and 80% for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features_baseline(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in baseline:\n",
    "        features[w] = (w in words)\n",
    "    return features \n",
    "\n",
    "def find_features_bigrams(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in bigram_features:\n",
    "        features[w] = (w in words)    \n",
    "    return features\n",
    "\n",
    "def find_features_content(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in content_features:\n",
    "        features[w] = (w in words)    \n",
    "    return features\n",
    "\n",
    "def find_features_bigrams_content(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in bigrams_content_features:\n",
    "        features[w] = (w in words)    \n",
    "    return features\n",
    "\n",
    "featuresets_baseline = [(find_features_baseline(rev), category) for (rev, category) in out]\n",
    "featuresets_bigrams = [(find_features_bigrams(rev), category) for (rev, category) in out]\n",
    "featuresets_content = [(find_features_content(rev), category) for (rev, category) in out]\n",
    "featuresets_bigrams_content = [(find_features_bigrams_content(rev), category) for (rev, category) in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algorithm accuracy of baseline: 0.7101588830043332\n"
     ]
    }
   ],
   "source": [
    "training_set_baseline = featuresets_baseline[:2077]\n",
    "testing_set_baseline = featuresets_baseline[:2077]\n",
    "classifier_baseline = nltk.NaiveBayesClassifier.train(training_set_baseline)\n",
    "print(\"Naive Bayes Algorithm accuracy of baseline:\", (nltk.classify.accuracy(classifier_baseline, testing_set_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algorithm accuracy of bigrams: 0.7580645161290323\n"
     ]
    }
   ],
   "source": [
    "training_set_bigrams = featuresets_bigrams[:2077]\n",
    "testing_set_bigrams = featuresets_bigrams[2077:]\n",
    "classifier_bigrams = nltk.NaiveBayesClassifier.train(training_set_bigrams)\n",
    "print(\"Naive Bayes Algorithm accuracy of bigrams:\", (nltk.classify.accuracy(classifier_bigrams, testing_set_bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algorithm accuracy of content words: 0.7032980259990371\n"
     ]
    }
   ],
   "source": [
    "training_set_content = featuresets_content[:2077]\n",
    "testing_set_content = featuresets_content[2077:]\n",
    "classifier_content = nltk.NaiveBayesClassifier.train(training_set_content)\n",
    "print(\"Naive Bayes Algorithm accuracy of content words:\", (nltk.classify.accuracy(classifier_content, testing_set_content)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algorithm accuracy of bigrams (only content words) 0.7580645161290323\n"
     ]
    }
   ],
   "source": [
    "training_set_bigrams_content = featuresets_bigrams_content[:2077]\n",
    "testing_set_bigrams_content = featuresets_bigrams_content[2077:]\n",
    "classifier_bigrams_content = nltk.NaiveBayesClassifier.train(training_set_bigrams_content)\n",
    "print(\"Naive Bayes Algorithm accuracy of bigrams (only content words)\", (nltk.classify.accuracy(classifier_bigrams_content, testing_set_bigrams_content)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
